# Importando pacotes
library(dplyr)
library(tidytext)
library(ggplot2)
library(tm)
library(stringi)
library(wordcloud)

# Lendo base
load("dados_rotulados.rda")

# Tirando colunas desnecessÃ¡rias
dados <- dados|>
  dplyr::select(text,Polaridade)

# Lendo arquivo de stopwords
stopwords <- read.delim("https://raw.githubusercontent.com/Claudionor20/XSentiment/main/stopwords_.txt",header = FALSE, stringsAsFactors = FALSE, encoding = "UTF-8")

# Drope duplicados
stopwords <- unique(stopwords)


# Transformando em vetor
stopwords <- as.vector(stopwords$V1)

# FunÃ§Ã£o de remoÃ§Ã£o de acentos
remove_acentos <- function(text) {
  text <- stri_trans_general(text, "latin-ascii")
  
  return(text)
}

# Criando dicionÃ¡rio de emojis
emojis <- c("â¤ï¸" = " amor ","ðŸ’—" = " amor ","ðŸ¥°" = " amor ","ðŸ’•" = " amor ","ðŸ’œ" = " amor ","ðŸ«¶ðŸ¾" = " amor ","â™¥" = " amor ","ðŸ’“" = " amor ","ðŸ‘ðŸ»" = " parabÃ©ns ",
            "ðŸ‘" = " parabÃ©ns ","ðŸ¥¹" = " amor ","ðŸ˜»" = " amor ","â¤ï¸â€ðŸ”¥" = " amor ","ðŸ˜˜" = " amor ","â£ï¸" = " amor ","ðŸ’Ÿ" = " amor ","ðŸ¤¢" = " nojo ",
            "ðŸ’–" = " amor ","ðŸ¤®" = " nojo ","ðŸ”¥" = " amor ","ðŸ–¤" = " amor ","ðŸ«¶ðŸ¼" = " amor ","ðŸ¤©" = " amor ","ðŸ’ž" = " amor ","ðŸ’" = " amor ","ðŸ˜Š" = " amor ")

# FunÃ§Ã£o de transformaÃ§Ã£o de emojis
substitute_emojis <- function(text, emoji_dict) {
  for (emoji in names(emoji_dict)) {
    text <- stri_replace_all_fixed(text, emoji, emoji_dict[[emoji]], vectorize_all = FALSE)
  }
  return(text)
}

# Criando corpus para utilizar a biblioteca tm
corpus <- Corpus(VectorSource(dados$text))

# Aplicando transformaÃ§Ãµes no corpus
corpus <- tm_map(corpus, removeWords, stopwords) # Removendo stopwords
corpus <- tm_map(corpus, content_transformer(substitute_emojis),emojis) # Substituindo emojis
corpus <- tm_map(corpus, content_transformer(remove_acentos)) # Removendo acentos
corpus <- tm_map(corpus, content_transformer(tolower)) # Transformando em minÃºsculo
corpus <- tm_map(corpus, removePunctuation) # Removendo pontuaÃ§Ã£o
corpus <- tm_map(corpus, removeNumbers) # Removendo nÃºmeros
corpus <- tm_map(corpus, stripWhitespace) # Removendo espaÃ§os em branco
corpus <- tm_map(corpus, removeWords, "anitta") # Removendo palavra "anitta"


# Transformando em matriz de termos
dtm <- DocumentTermMatrix(corpus)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wordcloud(names(freq), freq, min.freq=20)
dtm <- as.matrix(dtm)

# Fazendo grÃ¡fico de palavras mais frequentes
dados|>
  filter(Polaridade == 1)|>
  count(text, sort = TRUE)|>
  slice(1:40)|>
  ggplot(aes(x = reorder(text,n), y = n))+
  geom_col()+
  coord_flip()+
  labs(title = "Palavras mais frequentes",
       x = "Palavras",
       y = "FrequÃªncia")+
  theme_minimal()




